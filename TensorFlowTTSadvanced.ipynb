{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAQYOQuFa+okrVrAxOwdR2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoReis136/tensorflow/blob/main/TensorFlowTTSadvanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> TTS -  DOWNLOAD DO DATASET LJSPEECH </h1>"
      ],
      "metadata": {
        "id": "8mbBho6YF0lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "!tar xvjf LJSpeech-1.1.tar.bz2\n",
        "#download LJSpeech-1.1  dataset de TTS"
      ],
      "metadata": {
        "id": "YU-yuiIToAO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>CONVERTER ARQUIVOS DE ÁUDIO PARA PADRÕES RATE/CHANNEL/WIDTH</h2>"
      ],
      "metadata": {
        "id": "TSD88gWh1GX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "nNsFO69sysw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_dir = \"LJSpeech-1.1/wavs\"\n",
        "output_dir = \"LJSpeech-1.1/wavs16k\"\n",
        "\n",
        "for filename in tqdm(os.listdir(input_dir)):\n",
        "  if filename.endswith(\".wav\"):\n",
        "    filepath = os.path.join(input_dir, filename)\n",
        "    audio = AudioSegment.from_wav(filepath)\n",
        "\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
        "\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    audio.export(output_path, format=\"wav\")"
      ],
      "metadata": {
        "id": "tcsaUGLZy0I3",
        "outputId": "33ddd9bb-b422-4961-cbbe-f098f51e67fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13100/13100 [01:45<00:00, 123.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Dataframe dos metadatas e coluna com caminho dos arquivos</h2>"
      ],
      "metadata": {
        "id": "kjZddvjq23vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "metadata = pd.read_csv('LJSpeech-1.1/metadata.csv',sep=\"|\", header=None)\n",
        "metadata.columns = ['file_id', 'text', 'normalized_text']\n",
        "metadata['wav_path'] = metadata['file_id'].apply(lambda x:f'LJSpeech-1.1/wav16k/{x}.wav')"
      ],
      "metadata": {
        "id": "DxMIWk9s2rZl"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Extrair Mel com Librosa: FUNÇÃO</h2>"
      ],
      "metadata": {
        "id": "3TEfR-BP4AJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def convert_wav_to_mel(input_dir,\n",
        "                       output_dir,\n",
        "                       sr=16000,\n",
        "                       n_fft=1024,\n",
        "                       hop_length=256,\n",
        "                       n_mels=80,\n",
        "                       power=1.0,\n",
        "                       to_db=True,\n",
        "                       verbose=True):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    file_list = [f for f in os.listdir(input_dir) if f.endswith(\".wav\")]\n",
        "    iterator = tqdm(file_list, desc=\"Convertendo WAV → Mel\") if verbose else file_list\n",
        "\n",
        "    for filename in iterator:\n",
        "        filepath = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename.replace(\".wav\", \".npy\"))\n",
        "\n",
        "        y, _ = librosa.load(filepath, sr=sr)\n",
        "\n",
        "        mel = librosa.feature.melspectrogram(\n",
        "            y=y,\n",
        "            sr=sr,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            n_mels=n_mels,\n",
        "            power=power\n",
        "        )\n",
        "\n",
        "        if to_db:\n",
        "            mel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        np.save(output_path, mel)"
      ],
      "metadata": {
        "id": "6SkFHRn84V1-"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Extrair Mel com Librosa: EXECUÇÃO</h2>"
      ],
      "metadata": {
        "id": "Cahpmtk2sItT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_wav_to_mel(\n",
        "    input_dir=\"LJSpeech-1.1/wavs16k\",\n",
        "    output_dir=\"LJSpeech-1.1/mels\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDpNUShYsGXs",
        "outputId": "26001367-cbd8-4af6-f582-557f7b69bf8c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Convertendo WAV → Mel: 100%|██████████| 13100/13100 [03:38<00:00, 60.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Visualização do spectrograma (somente por diversão)</h2>"
      ],
      "metadata": {
        "id": "GcYoRifi5IKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "mel = np.load(\"LJSpeech-1.1/mels/LJ001-0001.npy\")\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mel, sr=16000, hop_length=256, x_axis=\"time\", y_axis=\"mel\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel Spectrogram\")\n",
        "plt.tight_layout()\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "A1r2jeXnrJ1M",
        "outputId": "526bb6c9-f7fb-410a-800e-671aab8c9994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import matplotlib.pyplot as plt\\nimport librosa.display\\n\\nmel = np.load(\"LJSpeech-1.1/mels/LJ001-0001.npy\")\\nplt.figure(figsize=(10, 4))\\nlibrosa.display.specshow(mel, sr=16000, hop_length=256, x_axis=\"time\", y_axis=\"mel\")\\nplt.colorbar(format=\"%+2.0f dB\")\\nplt.title(\"Mel Spectrogram\")\\nplt.tight_layout()\\nplt.show()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Dados tabulares para Dataframe e Pré Processamento</h2>"
      ],
      "metadata": {
        "id": "LflNfqS55c-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('LJSpeech-1.1/metadata.csv',\n",
        "                       sep=\"|\",\n",
        "                       header=None,\n",
        "                       names=['file_id', 'text', 'normalized_text'])\n",
        "\n",
        "df['normalized_text'].fillna(df['text'],inplace=True)\n",
        "df['normalized_text'].dropna(inplace=True)\n",
        "\n",
        "# Incluir coluna 'mel_path'\n",
        "df[\"mel_path\"] = df[\"file_id\"].apply(lambda x: f\"LJSpeech-1.1/mels/{x}.npy\")"
      ],
      "metadata": {
        "id": "cZHqu8vP5chG",
        "outputId": "dbfad13b-5320-4a44-f9e3-9a91e367324a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-75-368791196.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['normalized_text'].fillna(df['text'],inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tokenizar textos por BPE(byte pair encoding): Função</h2>"
      ],
      "metadata": {
        "id": "fVKgSkCS65-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "def train_sentencepiece(\n",
        "    texts,                 # lista de strings ou pd.Series com frases normalizadas\n",
        "    input_txt_path='temp_text.txt',  # arquivo temporário para salvar as frases\n",
        "    model_prefix='spm_model',        # prefixo do modelo e vocab\n",
        "    vocab_size=200                   # tamanho do vocabulário\n",
        "):\n",
        "    # Salva os textos em um arquivo temporário\n",
        "    with open(input_txt_path, 'w', encoding='utf-8') as f:\n",
        "        for line in texts:\n",
        "            f.write(line.strip() + '\\n')\n",
        "\n",
        "    # Treina o modelo SentencePiece\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=input_txt_path,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size\n",
        "    )\n",
        "\n",
        "    # (Opcional) Remove o arquivo temporário após treino\n",
        "    if os.path.exists(input_txt_path):\n",
        "        os.remove(input_txt_path)\n",
        "\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(f'{model_prefix}.model' )\n",
        "\n",
        "    # Adiciona ao DataFrame\n",
        "    df[\"tokens_bpe\"] = texts.apply(lambda t: sp.encode(t, out_type=int))"
      ],
      "metadata": {
        "id": "xYImb0oI7Aou"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tokenizar textos por BPE(byte pair encoding): Aplicação</h2>"
      ],
      "metadata": {
        "id": "VWt3mbvxtE-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentencepiece(\n",
        "    texts=df['normalized_text'],\n",
        "    input_txt_path='temp_text.txt',\n",
        "    model_prefix='spm_model',\n",
        "    vocab_size=700\n",
        ")"
      ],
      "metadata": {
        "id": "t0yyLwlgtELx"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Salva o dataframe como arquivo picke(.pkl)</h2>"
      ],
      "metadata": {
        "id": "ks4UYq_KDk4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_pickle('LJSpeech_preprocessed.pkl')"
      ],
      "metadata": {
        "id": "YAep2_wLwAjn"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Cria Classe Dataset para converter tokens/mels em tensores e retornar</h2>"
      ],
      "metadata": {
        "id": "pDixsVBnDxw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class TTSDataset(Dataset):\n",
        "  def __init__(self, dataframe, pad_token=0, max_input_length=None, max_mel_length=None):\n",
        "    self.df = dataframe\n",
        "    self.pad_token = pad_token\n",
        "    self.max_input_length = max_input_length\n",
        "    self.max_mel_length = max_mel_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df.iloc[idx]\n",
        "    tokens = row['tokens_bpe']\n",
        "    mel_path = row['mel_path']\n",
        "    # Carregar mel spec(.npy)\n",
        "    mel = np.load(mel_path)\n",
        "    #Converte em tensor\n",
        "    tokens = torch.LongTensor(tokens)\n",
        "    mel = torch.tensor(mel, dtype = torch.float32)\n",
        "\n",
        "    return tokens, mel"
      ],
      "metadata": {
        "id": "2MDnyIlY9vCd"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Função de padding dinâmico ao carregar os dados no DataLoader</h2>"
      ],
      "metadata": {
        "id": "2cQDdgO1EC-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tts_collate_fn(batch):\n",
        "    input_seqs, mel_specs = zip(*batch)\n",
        "\n",
        "    # Pad dos tokens\n",
        "    input_lengths = [len(seq) for seq in input_seqs]\n",
        "    max_input_length = max(input_lengths)\n",
        "    input_padded = torch.zeros(len(batch), max_input_length, dtype=torch.long)\n",
        "\n",
        "    for i, seq in enumerate(input_seqs):\n",
        "        input_padded[i, :len(seq)] = seq  # ✅ CORRETA\n",
        "\n",
        "    # Pad dos mel specs (formato = [n_mel, T])\n",
        "    mel_lengths = [mel.shape[1] for mel in mel_specs]\n",
        "    max_mel_len = max(mel_lengths)\n",
        "    n_mels = mel_specs[0].shape[0]\n",
        "    mel_padded = torch.zeros(len(batch), n_mels, max_mel_len)\n",
        "\n",
        "    for i, mel in enumerate(mel_specs):\n",
        "        mel_padded[i, :, :mel.shape[1]] = mel\n",
        "\n",
        "    mel_padded = mel_padded.transpose(1, 2)\n",
        "\n",
        "    print(\"input_ids_padded:\", input_padded.shape)\n",
        "    print(\"mel_padded:\", mel_padded.shape)\n",
        "\n",
        "    return input_padded, torch.tensor(input_lengths), mel_padded, torch.tensor(mel_lengths)"
      ],
      "metadata": {
        "id": "1Ehau_TPDkMr"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Carregando dados com DataLoader</h2>\n"
      ],
      "metadata": {
        "id": "B7WFataVIQEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Carregando Dataframe com token_bpe e mel_path\n",
        "# se necessário puxar o dataframe em df = pd.read_picke('LJSpeech_preprocessed')\n",
        "df = pd.read_pickle('LJSpeech_preprocessed.pkl')\n",
        "\n",
        "dataset = TTSDataset(df)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=tts_collate_fn)\n",
        "\n",
        "for batch in loader:\n",
        "    input_ids, input_lengths, mel_specs, mel_lengths = batch\n",
        "    print(input_ids.shape)      # [B, T_text]\n",
        "    print(mel_specs.shape)      # [B, 80, T_mel]\n",
        "    break"
      ],
      "metadata": {
        "id": "BzaQzh1uIWVn",
        "outputId": "d926de9c-5783-40cf-d84a-0e8636cae53f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 59])\n",
            "torch.Size([16, 80, 608])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Começando o modelo TACOTRON 2"
      ],
      "metadata": {
        "id": "Ko1RiQqq4HMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Dependências</h2>"
      ],
      "metadata": {
        "id": "vgq9-IhB7s4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Fyw2deot7tWb"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Classe Embedding + Encoder</h2>"
      ],
      "metadata": {
        "id": "hO2t6BYC4NbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, encoder_dim=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(embedding_dim, encoder_dim, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(encoder_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.lstm = nn.LSTM(encoder_dim, encoder_dim // 2, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        x = self.embedding(x)  # [B, T, E]\n",
        "        x = x.transpose(1, 2)  # [B, E, T]\n",
        "        x = self.conv1(x)      # [B, C, T]\n",
        "        x = x.transpose(1, 2)  # [B, T, C]\n",
        "        output, _ = self.lstm(x)\n",
        "        return output  # [B, T, C]\n"
      ],
      "metadata": {
        "id": "KpmRc2me4MU2"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Classe Attention</h2>"
      ],
      "metadata": {
        "id": "LChQDvku6PF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(decoder_dim, decoder_dim)\n",
        "        self.key_proj = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.energy_proj = nn.Linear(decoder_dim, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        # query: [B, decoder_dim]\n",
        "        # keys:  [B, T_enc, encoder_dim]\n",
        "        q = self.query_proj(query).unsqueeze(1)  # [B, 1, D]\n",
        "        k = self.key_proj(keys)                  # [B, T_enc, D]\n",
        "        energy = self.energy_proj(torch.tanh(q + k))  # [B, T_enc, 1]\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=-1)  # [B, T_enc]\n",
        "        context = torch.bmm(weights.unsqueeze(1), keys).squeeze(1)  # [B, encoder_dim]\n",
        "        return context, weights\n"
      ],
      "metadata": {
        "id": "q5t1dn616R7d"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Classe Decoder</h2>"
      ],
      "metadata": {
        "id": "Q9ACV3ur7mqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, encoder_dim, mel_dim=80, decoder_dim=1024):\n",
        "        super().__init__()\n",
        "        self.prenet = nn.Sequential(\n",
        "            nn.Linear(mel_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.attention = Attention(encoder_dim, decoder_dim)\n",
        "        self.lstm1 = nn.LSTMCell(256 + encoder_dim, decoder_dim)\n",
        "        self.linear = nn.Linear(decoder_dim, mel_dim)\n",
        "\n",
        "    def forward(self, encoder_out, mel_inputs, teacher_forcing=True):\n",
        "        B, T, _ = mel_inputs.shape\n",
        "        mel_outputs = []\n",
        "        attention_weights = []\n",
        "\n",
        "        h, c = [torch.zeros(B, 1024).to(encoder_out.device)] * 2\n",
        "\n",
        "        prev_mel = mel_inputs[:, 0, :]  # [B, mel_dim]\n",
        "\n",
        "        for t in range(1, T):\n",
        "            prenet_out = self.prenet(prev_mel)\n",
        "            context, attn = self.attention(h, encoder_out)\n",
        "            rnn_input = torch.cat([prenet_out, context], dim=-1)\n",
        "            h, c = self.lstm1(rnn_input, (h, c))\n",
        "            mel_out = self.linear(h)\n",
        "            mel_outputs.append(mel_out.unsqueeze(1))\n",
        "            attention_weights.append(attn.unsqueeze(1))\n",
        "            prev_mel = mel_inputs[:, t, :] if teacher_forcing else mel_out\n",
        "\n",
        "        mel_outputs = torch.cat(mel_outputs, dim=1)\n",
        "        attention_weights = torch.cat(attention_weights, dim=1)\n",
        "        return mel_outputs, attention_weights\n"
      ],
      "metadata": {
        "id": "mNzLVP__7oC_"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Postnet (Refinamento final)</h2>"
      ],
      "metadata": {
        "id": "RPvox_c39gZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Postnet(nn.Module):\n",
        "    def __init__(self, mel_dim=80):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(mel_dim, 512, kernel_size=5, padding=2),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv1d(512, mel_dim, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # [B, 80, T]\n",
        "        x = self.conv(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rOxf1rUF9iU4"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Modelo Tacotron 2</h2>"
      ],
      "metadata": {
        "id": "8UgWHNus9kA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size)\n",
        "        self.decoder = Decoder(encoder_dim=512)\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, input_ids, mel_inputs, teacher_forcing=True):\n",
        "        enc_out = self.encoder(input_ids)  # [B, T, 512]\n",
        "        mel_outputs, attn = self.decoder(enc_out, mel_inputs, teacher_forcing)\n",
        "        mel_refined = mel_outputs + self.postnet(mel_outputs)\n",
        "        return mel_outputs, mel_refined, attn\n"
      ],
      "metadata": {
        "id": "1hfhrawH9ppx"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Função de perda</h2>"
      ],
      "metadata": {
        "id": "ncq2nA_g-Fad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tacotron_loss(mel_outputs, mel_refined, mel_targets):\n",
        "    loss1 = F.l1_loss(mel_outputs, mel_targets)\n",
        "    loss2 = F.l1_loss(mel_refined, mel_targets)\n",
        "    return loss1 + loss2"
      ],
      "metadata": {
        "id": "8cjK0AHc-JWq"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Função de Treino </h2>"
      ],
      "metadata": {
        "id": "80IIv-zk-LRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, device, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "            input_ids, input_lengths, mel_specs, mel_lengths = batch\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            mel_specs = mel_specs.to(device)\n",
        "\n",
        "            # Forward\n",
        "            mel_out, mel_postnet, _ = model(input_ids, mel_specs)\n",
        "\n",
        "            # ✅ Ajuste de tamanhos\n",
        "            min_len = min(mel_out.shape[1], mel_specs.shape[1], mel_postnet.shape[1])\n",
        "            mel_out = mel_out[:, :min_len, :]\n",
        "            mel_postnet = mel_postnet[:, :min_len, :]\n",
        "            mel_specs = mel_specs[:, :min_len, :]\n",
        "\n",
        "            # Loss\n",
        "            loss = tacotron_loss(mel_out, mel_postnet, mel_specs)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "fg5f40Fr-K45"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Treinamento do modelo </h2>"
      ],
      "metadata": {
        "id": "Wf7BbgJe-Wgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"spm_model.model\")\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modelo\n",
        "model = Tacotron2(vocab_size=len(sp))  # sp = seu SentencePieceProcessor\n",
        "model = model.to(device)\n",
        "\n",
        "# Otimizador\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    collate_fn=tts_collate_fn  # já implementado\n",
        ")\n",
        "\n",
        "# Iniciar treino\n",
        "train(model, dataloader, optimizer, device, num_epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "lECyUF4n-ba0",
        "outputId": "dba17e98-f1cb-4f70-b926-e52f3901048f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/819 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([16, 62])\n",
            "mel_padded: torch.Size([16, 622, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-88-2883914882.py:2: UserWarning: Using a target size (torch.Size([16, 622, 80])) that is different to the input size (torch.Size([16, 621, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss1 = F.l1_loss(mel_outputs, mel_targets)\n",
            "\rEpoch 1:   0%|          | 0/819 [00:30<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (621) must match the size of tensor b (622) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-107-1649834258.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Iniciar treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-102-3949942665.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtacotron_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_postnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_specs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-88-2883914882.py\u001b[0m in \u001b[0;36mtacotron_loss\u001b[0;34m(mel_outputs, mel_refined, mel_targets)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtacotron_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_refined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_refined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3808\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (621) must match the size of tensor b (622) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}