{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq7TpqW056AIpMpOYVF/Wi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoReis136/tensorflow/blob/main/TensorFlowTTSadvanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> TTS -  DOWNLOAD DO DATASET LJSPEECH </h1>"
      ],
      "metadata": {
        "id": "8mbBho6YF0lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "!tar xvjf LJSpeech-1.1.tar.bz2\n",
        "#download LJSpeech-1.1  dataset de TTS"
      ],
      "metadata": {
        "id": "YU-yuiIToAO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>CONVERTER ARQUIVOS DE ÁUDIO PARA PADRÕES RATE/CHANNEL/WIDTH</h2>"
      ],
      "metadata": {
        "id": "TSD88gWh1GX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "nNsFO69sysw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "input_dir = \"LJSpeech-1.1/wavs\"\n",
        "output_dir = \"LJSpeech-1.1/wavs16k\"\n",
        "\n",
        "for filename in tqdm(os.listdir(input_dir)):\n",
        "  if filename.endswith(\".wav\"):\n",
        "    filepath = os.path.join(input_dir, filename)\n",
        "    audio = AudioSegment.from_wav(filepath)\n",
        "\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)\n",
        "\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    audio.export(output_path, format=\"wav\")"
      ],
      "metadata": {
        "id": "tcsaUGLZy0I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Extrair Mel com Librosa: FUNÇÃO</h2>"
      ],
      "metadata": {
        "id": "3TEfR-BP4AJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def convert_wav_to_mel(input_dir,\n",
        "                       output_dir,\n",
        "                       sr=16000,\n",
        "                       n_fft=1024,\n",
        "                       hop_length=256,\n",
        "                       n_mels=80,\n",
        "                       power=1.0,\n",
        "                       to_db=True,\n",
        "                       verbose=True):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    file_list = [f for f in os.listdir(input_dir) if f.endswith(\".wav\")]\n",
        "    iterator = tqdm(file_list, desc=\"Convertendo WAV → Mel\") if verbose else file_list\n",
        "\n",
        "    for filename in iterator:\n",
        "        filepath = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename.replace(\".wav\", \".npy\"))\n",
        "\n",
        "        y, _ = librosa.load(filepath, sr=sr)\n",
        "\n",
        "        mel = librosa.feature.melspectrogram(\n",
        "            y=y,\n",
        "            sr=sr,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            n_mels=n_mels,\n",
        "            power=power\n",
        "        )\n",
        "\n",
        "        if to_db:\n",
        "            mel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        np.save(output_path, mel)"
      ],
      "metadata": {
        "id": "6SkFHRn84V1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Extrair Mel com Librosa: EXECUÇÃO</h2>"
      ],
      "metadata": {
        "id": "Cahpmtk2sItT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_wav_to_mel(\n",
        "    input_dir=\"LJSpeech-1.1/wavs16k\",\n",
        "    output_dir=\"LJSpeech-1.1/mels\"\n",
        ")"
      ],
      "metadata": {
        "id": "qDpNUShYsGXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Visualização do spectrograma (somente por diversão)</h2>"
      ],
      "metadata": {
        "id": "GcYoRifi5IKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "mel = np.load(\"LJSpeech-1.1/mels/LJ001-0001.npy\")\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mel, sr=16000, hop_length=256, x_axis=\"time\", y_axis=\"mel\")\n",
        "plt.colorbar(format=\"%+2.0f dB\")\n",
        "plt.title(\"Mel Spectrogram\")\n",
        "plt.tight_layout()\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "A1r2jeXnrJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Com os dados prontos, pode reiniciar daqui:===========================================***"
      ],
      "metadata": {
        "id": "bHLUJ-yX-KjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Dados tabulares para Dataframe e Pré Processamento</h2>"
      ],
      "metadata": {
        "id": "LflNfqS55c-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "df = pd.read_csv('LJSpeech-1.1/metadata.csv',\n",
        "                       sep=\"|\",\n",
        "                       header=None,\n",
        "                       names=['file_id', 'text', 'normalized_text'])\n",
        "\n",
        "df['normalized_text'].fillna(df['text'],inplace=True)\n",
        "\n",
        "df['normalized_text'].dropna(inplace=True)\n",
        "\n",
        "df['wav_path'] = df['file_id'].apply(lambda x:f'LJSpeech-1.1/wav16k/{x}.wav')\n",
        "\n",
        "df[\"mel_path\"] = df[\"file_id\"].apply(lambda x: f\"LJSpeech-1.1/mels/{x}.npy\")"
      ],
      "metadata": {
        "id": "cZHqu8vP5chG",
        "outputId": "ff42f9cd-d15a-4026-9859-713a0bf50bac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-4110074409.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['normalized_text'].fillna(df['text'],inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tokenizar textos por BPE(byte pair encoding): Função</h2>"
      ],
      "metadata": {
        "id": "fVKgSkCS65-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import os\n",
        "\n",
        "def train_sentencepiece(\n",
        "    texts,                 # lista de strings ou pd.Series com frases normalizadas\n",
        "    input_txt_path='temp_text.txt',  # arquivo temporário para salvar as frases\n",
        "    model_prefix='spm_model',        # prefixo do modelo e vocab\n",
        "    vocab_size=200                   # tamanho do vocabulário\n",
        "):\n",
        "    # Salva os textos em um arquivo temporário\n",
        "    with open(input_txt_path, 'w', encoding='utf-8') as f:\n",
        "        for line in texts:\n",
        "            f.write(line.strip() + '\\n')\n",
        "\n",
        "    # Treina o modelo SentencePiece\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=input_txt_path,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size\n",
        "    )\n",
        "\n",
        "    # (Opcional) Remove o arquivo temporário após treino\n",
        "    if os.path.exists(input_txt_path):\n",
        "        os.remove(input_txt_path)\n",
        "\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(f'{model_prefix}.model' )\n",
        "\n",
        "    # Adiciona ao DataFrame\n",
        "    df[\"tokens_bpe\"] = texts.apply(lambda t: sp.encode(t, out_type=int))"
      ],
      "metadata": {
        "id": "xYImb0oI7Aou"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Tokenizar textos por BPE(byte pair encoding): Aplicação</h2>"
      ],
      "metadata": {
        "id": "VWt3mbvxtE-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentencepiece(\n",
        "    texts=df['normalized_text'],\n",
        "    input_txt_path='temp_text.txt',\n",
        "    model_prefix='spm_model',\n",
        "    vocab_size=700\n",
        ")"
      ],
      "metadata": {
        "id": "t0yyLwlgtELx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Salva o dataframe como arquivo picke(.pkl)</h2>"
      ],
      "metadata": {
        "id": "ks4UYq_KDk4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_pickle('LJSpeech_preprocessed.pkl')"
      ],
      "metadata": {
        "id": "YAep2_wLwAjn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Cria Classe Dataset para converter tokens/mels em tensores e retornar</h2>"
      ],
      "metadata": {
        "id": "pDixsVBnDxw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TTSDataset(Dataset):\n",
        "    def __init__(self, dataframe, mel_dir=None, pad_token=0, max_input_length=None, max_mel_length=None):\n",
        "        self.df = dataframe.head(100)  # AQUI DEFINIMOS A QUANTIDADE DE AMOSTRAS DE ACORDO COM O PODER DE PROCESSAMENTO DO AMBIENTE\n",
        "        self.mel_dir = mel_dir\n",
        "        self.pad_token = pad_token\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_mel_length = max_mel_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        tokens = row['tokens_bpe']\n",
        "        filename = row['file_id'] + \".npy\"\n",
        "\n",
        "        mel_path = os.path.join(self.mel_dir, filename) if self.mel_dir else row['mel_path']\n",
        "        mel = np.load(mel_path)\n",
        "\n",
        "        tokens = torch.LongTensor(tokens)\n",
        "        mel = torch.tensor(mel, dtype=torch.float32)\n",
        "\n",
        "        return tokens, mel"
      ],
      "metadata": {
        "id": "2MDnyIlY9vCd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Função de padding dinâmico ao carregar os dados no DataLoader</h2>"
      ],
      "metadata": {
        "id": "2cQDdgO1EC-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tts_collate_fn(batch):\n",
        "    input_seqs, mel_specs = zip(*batch)\n",
        "\n",
        "    # Pad dos tokens\n",
        "    input_lengths = [len(seq) for seq in input_seqs]\n",
        "    max_input_length = max(input_lengths)\n",
        "    input_padded = torch.zeros(len(batch), max_input_length, dtype=torch.long)\n",
        "\n",
        "    for i, seq in enumerate(input_seqs):\n",
        "        input_padded[i, :len(seq)] = seq  # ✅ CORRETA\n",
        "\n",
        "    # Pad dos mel specs (formato = [n_mel, T])\n",
        "    mel_lengths = [mel.shape[1] for mel in mel_specs]\n",
        "    max_mel_len = max(mel_lengths)\n",
        "    n_mels = mel_specs[0].shape[0]\n",
        "    mel_padded = torch.zeros(len(batch), n_mels, max_mel_len)\n",
        "\n",
        "    for i, mel in enumerate(mel_specs):\n",
        "        mel_padded[i, :, :mel.shape[1]] = mel\n",
        "\n",
        "    mel_padded = mel_padded.transpose(1, 2)\n",
        "\n",
        "    print(\"input_ids_padded:\", input_padded.shape)\n",
        "    print(\"mel_padded:\", mel_padded.shape)\n",
        "\n",
        "    return input_padded, torch.tensor(input_lengths), mel_padded, torch.tensor(mel_lengths)"
      ],
      "metadata": {
        "id": "1Ehau_TPDkMr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Carregando dados com DataLoader</h2>\n"
      ],
      "metadata": {
        "id": "B7WFataVIQEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Carregando Dataframe com token_bpe e mel_path\n",
        "# se necessário puxar o dataframe em df = pd.read_picke('LJSpeech_preprocessed')\n",
        "df = pd.read_pickle('LJSpeech_preprocessed.pkl')\n",
        "\n",
        "dataset = TTSDataset(df)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=tts_collate_fn)\n",
        "\n",
        "for batch in loader:\n",
        "    input_ids, input_lengths, mel_specs, mel_lengths = batch\n",
        "    print(input_ids.shape)      # [B, T_text]\n",
        "    print(mel_specs.shape)      # [B, 80, T_mel]\n",
        "    break"
      ],
      "metadata": {
        "id": "BzaQzh1uIWVn",
        "outputId": "f04e1626-8d77-4cf6-f71c-03221965fa57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([16, 65])\n",
            "mel_padded: torch.Size([16, 605, 80])\n",
            "torch.Size([16, 65])\n",
            "torch.Size([16, 605, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Começando o modelo TACOTRON 2"
      ],
      "metadata": {
        "id": "Ko1RiQqq4HMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Dependências</h2>"
      ],
      "metadata": {
        "id": "vgq9-IhB7s4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Fyw2deot7tWb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Classe Embedding + Encoder</h2>"
      ],
      "metadata": {
        "id": "hO2t6BYC4NbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, encoder_dim=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(embedding_dim, encoder_dim, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(encoder_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.lstm = nn.LSTM(encoder_dim, encoder_dim // 2, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        x = self.embedding(x)  # [B, T, E]\n",
        "        x = x.transpose(1, 2)  # [B, E, T]\n",
        "        x = self.conv1(x)      # [B, C, T]\n",
        "        x = x.transpose(1, 2)  # [B, T, C]\n",
        "        output, _ = self.lstm(x)\n",
        "        return output  # [B, T, C]\n"
      ],
      "metadata": {
        "id": "KpmRc2me4MU2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Classe Attention</h2>"
      ],
      "metadata": {
        "id": "LChQDvku6PF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(decoder_dim, decoder_dim)\n",
        "        self.key_proj = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.energy_proj = nn.Linear(decoder_dim, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        # query: [B, decoder_dim]\n",
        "        # keys:  [B, T_enc, encoder_dim]\n",
        "        q = self.query_proj(query).unsqueeze(1)  # [B, 1, D]\n",
        "        k = self.key_proj(keys)                  # [B, T_enc, D]\n",
        "        energy = self.energy_proj(torch.tanh(q + k))  # [B, T_enc, 1]\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=-1)  # [B, T_enc]\n",
        "        context = torch.bmm(weights.unsqueeze(1), keys).squeeze(1)  # [B, encoder_dim]\n",
        "        return context, weights\n"
      ],
      "metadata": {
        "id": "q5t1dn616R7d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Classe Decoder</h2>"
      ],
      "metadata": {
        "id": "Q9ACV3ur7mqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, encoder_dim, mel_dim=80, decoder_dim=1024):\n",
        "        super().__init__()\n",
        "        self.prenet = nn.Sequential(\n",
        "            nn.Linear(mel_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.attention = Attention(encoder_dim, decoder_dim)\n",
        "        self.lstm1 = nn.LSTMCell(256 + encoder_dim, decoder_dim)\n",
        "        self.linear = nn.Linear(decoder_dim, mel_dim)\n",
        "\n",
        "    def forward(self, encoder_out, mel_inputs, teacher_forcing=True):\n",
        "        B, T, _ = mel_inputs.shape\n",
        "        mel_outputs = []\n",
        "        attention_weights = []\n",
        "\n",
        "        h, c = [torch.zeros(B, 1024).to(encoder_out.device)] * 2\n",
        "\n",
        "        prev_mel = mel_inputs[:, 0, :]  # [B, mel_dim]\n",
        "\n",
        "        for t in range(1, T):\n",
        "            prenet_out = self.prenet(prev_mel)\n",
        "            context, attn = self.attention(h, encoder_out)\n",
        "            rnn_input = torch.cat([prenet_out, context], dim=-1)\n",
        "            h, c = self.lstm1(rnn_input, (h, c))\n",
        "            mel_out = self.linear(h)\n",
        "            mel_outputs.append(mel_out.unsqueeze(1))\n",
        "            attention_weights.append(attn.unsqueeze(1))\n",
        "            prev_mel = mel_inputs[:, t, :] if teacher_forcing else mel_out\n",
        "\n",
        "        mel_outputs = torch.cat(mel_outputs, dim=1)\n",
        "        attention_weights = torch.cat(attention_weights, dim=1)\n",
        "        return mel_outputs, attention_weights\n"
      ],
      "metadata": {
        "id": "mNzLVP__7oC_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Postnet (Refinamento final)</h2>"
      ],
      "metadata": {
        "id": "RPvox_c39gZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Postnet(nn.Module):\n",
        "    def __init__(self, mel_dim=80):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(mel_dim, 512, kernel_size=5, padding=2),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv1d(512, mel_dim, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # [B, 80, T]\n",
        "        x = self.conv(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rOxf1rUF9iU4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Modelo Tacotron 2</h2>"
      ],
      "metadata": {
        "id": "8UgWHNus9kA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size)\n",
        "        self.decoder = Decoder(encoder_dim=512)\n",
        "        self.postnet = Postnet()\n",
        "\n",
        "    def forward(self, input_ids, mel_inputs, teacher_forcing=True):\n",
        "        enc_out = self.encoder(input_ids)  # [B, T, 512]\n",
        "        mel_outputs, attn = self.decoder(enc_out, mel_inputs, teacher_forcing)\n",
        "        mel_refined = mel_outputs + self.postnet(mel_outputs)\n",
        "        return mel_outputs, mel_refined, attn\n"
      ],
      "metadata": {
        "id": "1hfhrawH9ppx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Função de perda</h2>"
      ],
      "metadata": {
        "id": "ncq2nA_g-Fad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tacotron_loss(mel_outputs, mel_refined, mel_targets):\n",
        "    loss1 = F.l1_loss(mel_outputs, mel_targets)\n",
        "    loss2 = F.l1_loss(mel_refined, mel_targets)\n",
        "    return loss1 + loss2"
      ],
      "metadata": {
        "id": "8cjK0AHc-JWq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Função de Treino </h2>"
      ],
      "metadata": {
        "id": "80IIv-zk-LRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, device, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "            input_ids, input_lengths, mel_specs, mel_lengths = batch\n",
        "\n",
        "            input_ids = input_ids.to(device)\n",
        "            mel_specs = mel_specs.to(device)\n",
        "\n",
        "            # Forward\n",
        "            mel_out, mel_postnet, _ = model(input_ids, mel_specs)\n",
        "\n",
        "            # ✅ Ajuste de tamanhos\n",
        "            min_len = min(mel_out.shape[1], mel_specs.shape[1], mel_postnet.shape[1])\n",
        "            mel_out = mel_out[:, :min_len, :]\n",
        "            mel_postnet = mel_postnet[:, :min_len, :]\n",
        "            mel_specs = mel_specs[:, :min_len, :]\n",
        "\n",
        "            # Loss\n",
        "            loss = tacotron_loss(mel_out, mel_postnet, mel_specs)\n",
        "\n",
        "            # Backprop\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "fg5f40Fr-K45"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Treinamento do modelo </h2>"
      ],
      "metadata": {
        "id": "Wf7BbgJe-Wgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"spm_model.model\")\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modelo\n",
        "model = Tacotron2(vocab_size=len(sp))  # sp = seu SentencePieceProcessor\n",
        "model = model.to(device)\n",
        "\n",
        "# Otimizador\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    collate_fn=tts_collate_fn  # já implementado\n",
        ")\n",
        "\n",
        "# Iniciar treino\n",
        "train(model, dataloader, optimizer, device, num_epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lECyUF4n-ba0",
        "outputId": "751d5814-f06a-4256-afb2-f050531e034d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/13 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 52])\n",
            "mel_padded: torch.Size([8, 608, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   8%|▊         | 1/13 [00:36<07:15, 36.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 65])\n",
            "mel_padded: torch.Size([8, 622, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  15%|█▌        | 2/13 [01:16<07:05, 38.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 58])\n",
            "mel_padded: torch.Size([8, 605, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  23%|██▎       | 3/13 [01:52<06:13, 37.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 56])\n",
            "mel_padded: torch.Size([8, 571, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  31%|███       | 4/13 [02:25<05:21, 35.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 59])\n",
            "mel_padded: torch.Size([8, 586, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  38%|███▊      | 5/13 [03:01<04:46, 35.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 49])\n",
            "mel_padded: torch.Size([8, 515, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  46%|████▌     | 6/13 [03:30<03:53, 33.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 58])\n",
            "mel_padded: torch.Size([8, 567, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  54%|█████▍    | 7/13 [04:04<03:22, 33.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 51])\n",
            "mel_padded: torch.Size([8, 539, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  62%|██████▏   | 8/13 [04:41<02:53, 34.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 57])\n",
            "mel_padded: torch.Size([8, 537, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  69%|██████▉   | 9/13 [05:16<02:18, 34.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 66])\n",
            "mel_padded: torch.Size([8, 605, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  77%|███████▋  | 10/13 [05:55<01:48, 36.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 50])\n",
            "mel_padded: torch.Size([8, 555, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  85%|████████▍ | 11/13 [06:27<01:09, 34.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 55])\n",
            "mel_padded: torch.Size([8, 603, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  92%|█████████▏| 12/13 [07:05<00:35, 35.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([4, 65])\n",
            "mel_padded: torch.Size([4, 600, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [07:35<00:00, 35.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 26.9691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:   0%|          | 0/13 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 49])\n",
            "mel_padded: torch.Size([8, 568, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:   8%|▊         | 1/13 [00:34<06:55, 34.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 55])\n",
            "mel_padded: torch.Size([8, 578, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  15%|█▌        | 2/13 [01:11<06:37, 36.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 59])\n",
            "mel_padded: torch.Size([8, 531, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  23%|██▎       | 3/13 [01:47<05:59, 35.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 66])\n",
            "mel_padded: torch.Size([8, 605, 80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  31%|███       | 4/13 [02:34<06:04, 40.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids_padded: torch.Size([8, 49])\n",
            "mel_padded: torch.Size([8, 603, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Salvar Modelo </h2>"
      ],
      "metadata": {
        "id": "pUQbNgneERsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"model\", exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), \"model/tacotron2_weights.pth\")"
      ],
      "metadata": {
        "id": "tRxHuP7DERGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Recarregando o modelo (quando for o caso) </h2>"
      ],
      "metadata": {
        "id": "Sezn1vE5E3Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "from tacotron2 import Tacotron2  # substitua pelo caminho real da sua classe\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assumindo que você treinou com SentencePiece ou similar\n",
        "vocab_size = len(sp)  # seu SentencePieceProcessor deve estar carregado\n",
        "\n",
        "# Criar e carregar modelo\n",
        "model = Tacotron2(vocab_size=vocab_size)\n",
        "model.load_state_dict(torch.load(\"model/tacotron2_weights.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Modelo carregado e pronto para inferência.\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "IlN4cU9pE3qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Função pra utilizar o modelo </h1>"
      ],
      "metadata": {
        "id": "IJg8zid4FFY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf  # para salvar WAV\n",
        "\n",
        "def tts_infer(text, model, sp, device, sr=16000, n_fft=1024, hop_length=256):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenizar entrada\n",
        "    tokens = sp.encode(text, out_type=int)\n",
        "    tokens = torch.LongTensor(tokens).unsqueeze(0).to(device)  # [1, T]\n",
        "\n",
        "    # Rodar modelo (sem professor forcing)\n",
        "    with torch.no_grad():\n",
        "        mel_out, mel_postnet, _ = model(tokens)\n",
        "\n",
        "    # Pegar saída pós-net\n",
        "    mel = mel_postnet.squeeze(0).cpu().numpy()  # [n_mels, T]\n",
        "\n",
        "    # Reconstruir áudio com Griffin-Lim\n",
        "    wav = librosa.feature.inverse.mel_to_audio(\n",
        "        mel,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        power=1.0\n",
        "    )\n",
        "\n",
        "    # Salvar áudio\n",
        "    output_path = \"output.wav\"\n",
        "    sf.write(output_path, wav, sr)\n",
        "    print(f\"✅ Áudio gerado e salvo como: {output_path}\")\n"
      ],
      "metadata": {
        "id": "pO6VyslSFFAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Input de texto e saída de áudio </h2>"
      ],
      "metadata": {
        "id": "aRyNLQAKFRT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = input(\"Escreva um texto pra gerar um áudio\")\n",
        "\n",
        "tts_infer(texto, model, sp, device)"
      ],
      "metadata": {
        "id": "yMFKCO1eFcnt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}