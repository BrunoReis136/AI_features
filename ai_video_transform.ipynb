{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoReis136/AI_features/blob/main/ai_video_transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß∞ Instalar depend√™ncias\n",
        "!pip install diffusers transformers accelerate scipy safetensors opencv-python moviepy\n",
        "\n",
        "# üß± Importa√ß√µes\n",
        "import os\n",
        "import cv2\n",
        "from moviepy.editor import *\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# üìÅ Criar diret√≥rios\n",
        "os.makedirs(\"frames_original\", exist_ok=True)\n",
        "os.makedirs(\"frames_transformados\", exist_ok=True)\n",
        "\n",
        "# üìΩÔ∏è Upload do v√≠deo\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# üéûÔ∏è Extrair frames\n",
        "vidcap = cv2.VideoCapture(video_path)\n",
        "success, image = vidcap.read()\n",
        "count = 0\n",
        "while success:\n",
        "    cv2.imwrite(f\"frames_original/frame{count:03d}.png\", image)\n",
        "    success, image = vidcap.read()\n",
        "    count += 1\n",
        "print(f\"{count} frames extra√≠dos.\")\n",
        "\n",
        "# üì¶ Baixar modelo ControlNet\n",
        "from diffusers.utils import load_image\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet=controlnet,\n",
        "    safety_checker=None,\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# üé® Fun√ß√£o para transformar cada frame\n",
        "from PIL import ImageFilter\n",
        "\n",
        "prompt = input(\"\\n Descreva as mudan√ßas no v√≠deo: \\n\")\n",
        "negative_prompt = \"blurry, low quality, deformed, text, watermark\"\n",
        "\n",
        "for i in range(count):\n",
        "    img = Image.open(f\"frames_original/frame{i:03d}.png\")\n",
        "    img = img.resize((512, 512))  # redimensiona para SD\n",
        "    img = img.filter(ImageFilter.EDGE_ENHANCE)  # ou canny se quiser\n",
        "    output = pipe(prompt, image=img, num_inference_steps=25, negative_prompt=negative_prompt).images[0]\n",
        "    output.save(f\"frames_transformados/frame{i:03d}.png\")\n",
        "\n",
        "# üéûÔ∏è Juntar os frames transformados em v√≠deo\n",
        "image_files = [f\"frames_transformados/frame{i:03d}.png\" for i in range(count)]\n",
        "clip = ImageSequenceClip(image_files, fps=vidcap.get(cv2.CAP_PROP_FPS))\n",
        "clip.write_videofile(\"video_sinistro.mp4\", codec=\"libx264\")\n"
      ],
      "metadata": {
        "id": "MvB9zGJr-IzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß∞ Instalar depend√™ncias\n",
        "!pip install opencv-python moviepy\n",
        "\n",
        "# üß± Importa√ß√µes\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "from google.colab import files\n",
        "\n",
        "# üìÅ Criar diret√≥rios\n",
        "os.makedirs(\"frames_original\", exist_ok=True)\n",
        "os.makedirs(\"frames_transformados\", exist_ok=True)\n",
        "\n",
        "# üì§ Upload do v√≠deo e personagem\n",
        "print(\"Fa√ßa upload do V√çDEO e da IMAGEM PNG do personagem com fundo transparente.\")\n",
        "uploaded = files.upload()\n",
        "video_path = [f for f in uploaded.keys() if f.endswith(\".mp4\")][0]\n",
        "personagem_path = [f for f in uploaded.keys() if f.endswith(\".png\")][0]\n",
        "\n",
        "# üéûÔ∏è Extrair frames do v√≠deo\n",
        "vidcap = cv2.VideoCapture(video_path)\n",
        "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "success, image = vidcap.read()\n",
        "count = 0\n",
        "while success:\n",
        "    cv2.imwrite(f\"frames_original/frame{count:03d}.png\", image)\n",
        "    success, image = vidcap.read()\n",
        "    count += 1\n",
        "print(f\"{count} frames extra√≠dos.\")\n",
        "\n",
        "# üßü‚Äç‚ôÇÔ∏è Carregar personagem com fundo transparente\n",
        "personagem = Image.open(personagem_path).convert(\"RGBA\")\n",
        "personagem = personagem.resize((150, 300))  # ajuste conforme desejar\n",
        "\n",
        "# üå´Ô∏è Fun√ß√£o para aplicar neblina no frame (OpenCV)\n",
        "def aplicar_neblina_cv2(frame_np):\n",
        "    h, w, _ = frame_np.shape\n",
        "    neblina = np.random.normal(loc=200, scale=30, size=(h, w, 3)).astype(np.uint8)\n",
        "    neblina = cv2.GaussianBlur(neblina, (101, 101), 0)\n",
        "    return cv2.addWeighted(frame_np, 0.85, neblina, 0.15, 0)\n",
        "\n",
        "# üñºÔ∏è Processar cada frame\n",
        "for i in range(count):\n",
        "    # Carregar frame original\n",
        "    frame_path = f\"frames_original/frame{i:03d}.png\"\n",
        "    frame = Image.open(frame_path).convert(\"RGBA\")\n",
        "\n",
        "    # Adicionar personagem (simula movimento vindo de longe)\n",
        "    x = 200  # posi√ß√£o horizontal fixa\n",
        "    y = 400 - i * 4  # personagem se aproxima\n",
        "    frame_editado = frame.copy()\n",
        "    frame_editado.paste(personagem, (x, y), personagem)\n",
        "\n",
        "    # Converter para NumPy para aplicar neblina com OpenCV\n",
        "    frame_np = np.array(frame_editado.convert(\"RGB\"))\n",
        "    frame_np = aplicar_neblina_cv2(frame_np)\n",
        "\n",
        "    # Salvar frame final\n",
        "    final = Image.fromarray(frame_np)\n",
        "    final.save(f\"frames_transformados/frame{i:03d}.png\")\n",
        "\n",
        "print(\"‚úÖ Todos os frames foram processados.\")\n",
        "\n",
        "# üìΩÔ∏è Recriar v√≠deo final\n",
        "image_files = [f\"frames_transformados/frame{i:03d}.png\" for i in range(count)]\n",
        "clip = ImageSequenceClip(image_files, fps=fps)\n",
        "clip.write_videofile(\"video_sinistro_final.mp4\", codec=\"libx264\")\n",
        "\n",
        "print(\"‚úÖ V√≠deo gerado com sucesso: video_sinistro_final.mp4\")\n"
      ],
      "metadata": {
        "id": "WhS1ALwEM8gF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Conhe√ßa o Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}