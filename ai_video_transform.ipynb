{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunoReis136/AI_features/blob/main/ai_video_transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß∞ Instalar depend√™ncias\n",
        "!pip install diffusers transformers accelerate scipy safetensors opencv-python moviepy\n",
        "\n",
        "# üß± Importa√ß√µes\n",
        "import os\n",
        "import cv2\n",
        "from moviepy.editor import *\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# üìÅ Criar diret√≥rios\n",
        "os.makedirs(\"frames_original\", exist_ok=True)\n",
        "os.makedirs(\"frames_transformados\", exist_ok=True)\n",
        "\n",
        "# üìΩÔ∏è Upload do v√≠deo\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# üéûÔ∏è Extrair frames\n",
        "vidcap = cv2.VideoCapture(video_path)\n",
        "success, image = vidcap.read()\n",
        "count = 0\n",
        "while success:\n",
        "    cv2.imwrite(f\"frames_original/frame{count:03d}.png\", image)\n",
        "    success, image = vidcap.read()\n",
        "    count += 1\n",
        "print(f\"{count} frames extra√≠dos.\")\n",
        "\n",
        "# üì¶ Baixar modelo ControlNet\n",
        "from diffusers.utils import load_image\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet=controlnet,\n",
        "    safety_checker=None,\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# üé® Fun√ß√£o para transformar cada frame\n",
        "from PIL import ImageFilter\n",
        "\n",
        "prompt = input(\"\\n Descreva as mudan√ßas no v√≠deo: \\n\")\n",
        "negative_prompt = \"blurry, low quality, deformed, text, watermark\"\n",
        "\n",
        "for i in range(count):\n",
        "    img = Image.open(f\"frames_original/frame{i:03d}.png\")\n",
        "    img = img.resize((512, 512))  # redimensiona para SD\n",
        "    img = img.filter(ImageFilter.EDGE_ENHANCE)  # ou canny se quiser\n",
        "    output = pipe(prompt, image=img, num_inference_steps=25, negative_prompt=negative_prompt).images[0]\n",
        "    output.save(f\"frames_transformados/frame{i:03d}.png\")\n",
        "\n",
        "# üéûÔ∏è Juntar os frames transformados em v√≠deo\n",
        "image_files = [f\"frames_transformados/frame{i:03d}.png\" for i in range(count)]\n",
        "clip = ImageSequenceClip(image_files, fps=vidcap.get(cv2.CAP_PROP_FPS))\n",
        "clip.write_videofile(\"video_sinistro.mp4\", codec=\"libx264\")\n"
      ],
      "metadata": {
        "id": "MvB9zGJr-IzZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Conhe√ßa o Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}